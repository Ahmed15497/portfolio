{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXJRDnWoa5gf"
      },
      "source": [
        "# **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dv5opgTrsLN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset"
      ],
      "metadata": {
        "id": "y8LJ6epTJhRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/datajpg/train.csv /content/dataset"
      ],
      "metadata": {
        "id": "wvbPxrpMJs8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePj-gnO0IB74"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynf5Ki9UPQaB"
      },
      "outputs": [],
      "source": [
        "!pip install imgaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNEGRs3mHar1"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from skimage.filters import threshold_otsu\n",
        "import imgaug.augmenters as iaa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJTZO0WrMlPq"
      },
      "source": [
        "# **Downloading dataset from website**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/dataset"
      ],
      "metadata": {
        "id": "gKPQy5u5J5FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvGQ8LXZM5lE"
      },
      "outputs": [],
      "source": [
        "!wget \"https://isic-challenge-data.s3.amazonaws.com/2020/ISIC_2020_Training_JPEG.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gri1dY_0bD77"
      },
      "source": [
        "# **Unzipping dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaMVGYHjP93V"
      },
      "outputs": [],
      "source": [
        "#!mkdir dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vUXs7IlQFSK"
      },
      "outputs": [],
      "source": [
        "#%cd dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY6RsgMmQNDE"
      },
      "outputs": [],
      "source": [
        "#!mv /content/ISIC_2020_Training_JPEG.zip /content/dataset/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FDpGzzdPisU"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/dataset/ISIC_2020_Training_JPEG.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KaiS7ScKIvl"
      },
      "outputs": [],
      "source": [
        "#!unzip \"/content/dataset/train.csv.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ_o99fOKJAR"
      },
      "outputs": [],
      "source": [
        "main_path = \"/content/dataset/train\"\n",
        "len(os.listdir(main_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hysl9WRCPGQv"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/dataset/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjChYoeMT6fA"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qDozbtGPtKO"
      },
      "outputs": [],
      "source": [
        "df['image_name_'] = df['image_name'].apply(lambda x: f\"{main_path}/{x}.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awQtIX1RUGr7"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "jJO9EQQJm8tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((8, 8, 256)))\n",
        "    assert model.output_shape == (None, 8, 8, 256)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 8, 8, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 16, 16, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 32, 32, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 64, 64, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 128, 128, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 256, 256, 3)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "RBLcZKPv4dxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()\n"
      ],
      "metadata": {
        "id": "xvZHTaxpwb8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.summary()"
      ],
      "metadata": {
        "id": "_gwZFWpcwdCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)"
      ],
      "metadata": {
        "id": "EqBhH7bqRzO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(generated_image)"
      ],
      "metadata": {
        "id": "xcd-aFx2D6zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[256, 256, 3]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "7ltZ9Rv-SltC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = make_discriminator_model()\n"
      ],
      "metadata": {
        "id": "T9kB42t9xGm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.summary()"
      ],
      "metadata": {
        "id": "fLkGQgW7xPID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(discriminator.layers)"
      ],
      "metadata": {
        "id": "qPJvZTFrxaZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision = discriminator(generated_image)\n",
        "print(decision)"
      ],
      "metadata": {
        "id": "iKC9Zbm3TcSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "CbFR0KsQUiI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "tYpmElUYTh1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "qbQDhDt-Uky4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)"
      ],
      "metadata": {
        "id": "96ndBsndU_vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_models(generator, discriminator, epoch):\n",
        "  \"\"\" Save models at specific point in time. \"\"\"\n",
        "  tf.keras.models.save_model(\n",
        "    generator,\n",
        "    f'/content/drive/MyDrive/models/gan/generator.model2',\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        "  )\n",
        "  tf.keras.models.save_model(\n",
        "    discriminator,\n",
        "    f'/content/drive/MyDrive/models/gan/discriminator.model2',\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        "  )\n",
        "\n"
      ],
      "metadata": {
        "id": "I9arCk1s_uez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models(epoch):\n",
        "  generator = tf.keras.models.load_model(f'/content/drive/MyDrive/models/gan/generator.model2')\n",
        "  discriminator = tf.keras.models.load_model(f'/content/drive/MyDrive/models/gan/discriminator.model2')\n",
        "  return generator, discriminator"
      ],
      "metadata": {
        "id": "XirrjDE_KnWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator, discriminator = load_models(0)"
      ],
      "metadata": {
        "id": "Eb5GSqlrAqOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20000\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "metadata": {
        "id": "pGGqrmpyXOsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "metadata": {
        "id": "jeCSFMqzXa0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "x4LlauupZMEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  #fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      #plt.subplot(4, 4, i+1)\n",
        "      curr_pred = predictions[i]\n",
        "      curr_pred = curr_pred.numpy()\n",
        "      curr_pred = curr_pred * 127.5 + 127.5\n",
        "\n",
        "      #plt.imshow(curr_pred * 127.5 + 127.5)\n",
        "      #plt.axis('off')\n",
        "      filename = f'/content/drive/MyDrive/models/output_images/image{i}_at_epoch_{epoch}.png'\n",
        "      #print(curr_pred)\n",
        "      #print(filename)\n",
        "      if i in [1, 2]:\n",
        "        cv2_imshow(curr_pred)\n",
        "\n",
        "  cv2.imwrite(filename, curr_pred)\n",
        "\n",
        "  #plt.savefig('/content/drive/MyDrive/models/output_images/image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "7Vf0lD8taEdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch, labels in dataset:\n",
        "      #print(image_batch.shape)\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    #display.clear_output(wait=True)\n",
        "\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch,\n",
        "                             seed)\n",
        "\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      save_models(generator, discriminator, epoch)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  #display.clear_output(wait=True)\n",
        "\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)\n"
      ],
      "metadata": {
        "id": "HhU6R3_5Y0bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "  'Generates data for Keras'\n",
        "  def __init__(self, list_IDs, labels, batch_size=16, dim=(256,256), n_channels=3,\n",
        "              n_classes=2, shuffle=True, augmentation=True, segmentation=True):\n",
        "    'Initialization'\n",
        "    self.dim = dim\n",
        "    self.batch_size = batch_size\n",
        "    self.labels = labels\n",
        "    self.list_IDs = list_IDs\n",
        "    self.n_channels = n_channels\n",
        "    self.n_classes = n_classes\n",
        "    self.shuffle = shuffle\n",
        "    self.augmentation = augmentation\n",
        "    self.segmentation = segmentation\n",
        "\n",
        "    if self.augmentation:\n",
        "      self.seq = iaa.Sequential([\n",
        "                              iaa.GaussianBlur(sigma=(0.1, 3.5)),\n",
        "                              iaa.Emboss(alpha=(0.0, 1.0), strength=(0.0, 1.5)),\n",
        "                              iaa.Fliplr(0.25),\n",
        "                              iaa.Flipud(0.25),\n",
        "                              iaa.Affine(rotate=(-45, 45)),\n",
        "                              iaa.PiecewiseAffine(scale=(0.01, 0.05)),\n",
        "                              iaa.Affine(shear=(-7, 7))\n",
        "                          ])\n",
        "\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the number of steps per epoch'\n",
        "    return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generate one batch of data'\n",
        "    # Generate indexes of the batch\n",
        "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "    # Find list of IDs\n",
        "    list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "    list_labels_temp = [self.labels[k] for k in indexes]\n",
        "\n",
        "    # Generate data\n",
        "    X, y = self.__data_generation(list_IDs_temp, list_labels_temp)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    'Updates indexes after each epoch'\n",
        "    self.indexes = np.arange(len(self.list_IDs))\n",
        "    if self.shuffle == True:\n",
        "        np.random.shuffle(self.indexes)\n",
        "\n",
        "  def hair_removal(self, src):\n",
        "    # Convert the original image to grayscale\n",
        "    #self.grayScale = cv2.cvtColor( src, cv2.COLOR_RGB2GRAY )\n",
        "    # Kernel for the morphological filtering\n",
        "    kernel = cv2.getStructuringElement(1,(17,17))\n",
        "    # Perform the blackHat filtering on the grayscale image to find the\n",
        "    # hair countours\n",
        "    blackhat = cv2.morphologyEx(self.grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
        "    # intensify the hair countours in preparation for the inpainting\n",
        "    # algorithm\n",
        "    ret,thresh2 = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n",
        "    # inpaint the original image depending on the mask\n",
        "    dst = cv2.inpaint(src,thresh2,1,cv2.INPAINT_TELEA)\n",
        "    return dst\n",
        "\n",
        "  def segment_image(self, image):\n",
        "    #self.grayScale = cv2.cvtColor( image, cv2.COLOR_RGB2GRAY )\n",
        "    th = threshold_otsu(self.grayScale)\n",
        "    mask  = self.grayScale < th\n",
        "    mask = np.stack((mask,)*3, axis=-1)\n",
        "    filtered = image * mask\n",
        "    return filtered\n",
        "\n",
        "\n",
        "  def augment_image(self, image):\n",
        "    aug_image = self.seq.augment_image(image)\n",
        "    return aug_image\n",
        "\n",
        "\n",
        "  def __data_generation(self, list_IDs_temp, list_labels_temp):\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "    # Initialization\n",
        "    X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "    y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "    # Generate data\n",
        "    for i, ID in enumerate(list_IDs_temp):\n",
        "        # Store sample\n",
        "        #img = cv2.imread(ID,cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.imread(ID)\n",
        "        #print(ID)\n",
        "        img_resized = cv2.resize(img, self.dim[::-1])\n",
        "        img_resized = cv2.medianBlur(img_resized, 3)\n",
        "        self.grayScale = cv2.cvtColor( img_resized, cv2.COLOR_RGB2GRAY )\n",
        "        img_resized = self.hair_removal(img_resized)\n",
        "        # Segmentation block\n",
        "        if self.segmentation:\n",
        "          img_resized = self.segment_image(img_resized)\n",
        "        # Classical augmentation\n",
        "        if self.augmentation:\n",
        "          img_resized = self.augment_image(img_resized)\n",
        "        # GAN augmentation\n",
        "\n",
        "\n",
        "        #img_resized = cv2.cvtColor( img_resized, cv2.COLOR_RGB2GRAY )\n",
        "        #img_resized = np.expand_dims(img_resized, axis=-1)\n",
        "        X[i, ] = img_resized\n",
        "        # Store class\n",
        "        y[i] = list_labels_temp[i]\n",
        "\n",
        "    return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n"
      ],
      "metadata": {
        "id": "g6rqglGocKdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_malg = df[df['target'] == 1]"
      ],
      "metadata": {
        "id": "ciXWBebKe0bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_malg"
      ],
      "metadata": {
        "id": "nTv6KU7NfOrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image = cv2.imread(df_malg['image_name_'].to_list()[7])\n",
        "#cv2_imshow(image)"
      ],
      "metadata": {
        "id": "8ctn9mbZ6LPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = DataGenerator(list_IDs= df_malg['image_name_'].to_list(),\n",
        "                                labels= df_malg['target'].to_list(),\n",
        "                                n_channels=3,\n",
        "                                shuffle=True,\n",
        "                                augmentation=True,\n",
        "                                batch_size=128,\n",
        "                                segmentation=False\n",
        "                                )"
      ],
      "metadata": {
        "id": "r0h2pYxHeX1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_generator:\n",
        "  for image in images:\n",
        "    cv2_imshow(image)\n",
        "  break"
      ],
      "metadata": {
        "id": "wMIEnAUqgaA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS"
      ],
      "metadata": {
        "id": "QC7MPOKggMj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_generator, EPOCHS)"
      ],
      "metadata": {
        "id": "2FjGnSNOY8Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal([20, 100])\n",
        "noise"
      ],
      "metadata": {
        "id": "w45A3A5T3HAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)"
      ],
      "metadata": {
        "id": "NwxTKSehEmKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image = generated_image.numpy()\n"
      ],
      "metadata": {
        "id": "CJH-WKNug7S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image_ = generated_image[0]\n",
        "generated_image_ = generated_image_ * 127.5 + 127.5"
      ],
      "metadata": {
        "id": "ckck4axV35Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(generated_image_)"
      ],
      "metadata": {
        "id": "Uqte7ar5hMss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image_ = generated_image[1]\n",
        "generated_image_ = generated_image_ * 127.5 + 127.5"
      ],
      "metadata": {
        "id": "jfMKKo5LhP7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(generated_image_)"
      ],
      "metadata": {
        "id": "eoon80Zz4OWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5Df0dpJ4Qbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}